import json

def calculate_evaluation_metrics(file_path):
    """
    Calculates evaluation metrics from a JSON file containing matched and unmatched aspects.

    Args:
        file_path (str): The path to the JSON file.

    Returns:
        dict: A dictionary containing the calculated metrics.
              Returns None if the file cannot be processed.
    """
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return None
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from {file_path}")
        return None

    matched_aspects = data.get("matched_aspects", [])
    unmatched_aspects_list_a = data.get("unmatched_aspects_list_a", [])
    unmatched_aspects_list_b = data.get("unmatched_aspects_list_b", [])

    num_matched_aspects = len(matched_aspects)
    num_unmatched_a = len(unmatched_aspects_list_a)
    num_unmatched_b = len(unmatched_aspects_list_b)

    total_confidence_score = 0
    for aspect in matched_aspects:
        total_confidence_score += aspect.get("confidence_score", 0)
    
    average_confidence_score = 0
    if num_matched_aspects > 0:
        average_confidence_score = total_confidence_score / num_matched_aspects

    # True Positives (TP): Number of matched aspects
    tp = num_matched_aspects
    # False Negatives (FN): Number of unmatched aspects in list A (reference aspects not matched in generated)
    fn = num_unmatched_a
    # False Positives (FP): Number of unmatched aspects in list B (generated aspects not matched in reference)
    fp = num_unmatched_b

    precision = 0
    if (tp + fp) > 0:
        precision = tp / (tp + fp)

    recall = 0
    if (tp + fn) > 0:
        recall = tp / (tp + fn)

    f1_score = 0
    if (precision + recall) > 0:
        f1_score = 2 * (precision * recall) / (precision + recall)
        
    metrics = {
        "num_matched_aspects": num_matched_aspects,
        "num_unmatched_aspects_list_a": num_unmatched_a,
        "num_unmatched_aspects_list_b": num_unmatched_b,
        "average_confidence_score": average_confidence_score,
        "true_positives (TP)": tp,
        "false_negatives (FN)": fn,
        "false_positives (FP)": fp,
        "precision": precision,
        "recall": recall,
        "f1_score": f1_score
    }

    return metrics

if __name__ == "__main__":
    # Assuming 'Prompt3b_Result.json' is in the same directory as the script
    file_path = "/Users/thomaspathe/Desktop/EVAL_MAL/QUALEval/exploitPublicFacing Application/Evaluation/Prompt3b_Result.json"
    
    results = calculate_evaluation_metrics(file_path)

    if results:
        print("\nEvaluation Metrics:")
        print("--------------------------------------------------")
        print(f"Number of Matched Aspects: {results['num_matched_aspects']}")
        print(f"Number of Unmatched Aspects (List A - Reference): {results['num_unmatched_aspects_list_a']}")
        print(f"Number of Unmatched Aspects (List B - Generated): {results['num_unmatched_aspects_list_b']}")
        print(f"Average Confidence Score of Matched Aspects: {results['average_confidence_score']:.4f}")
        print("--------------------------------------------------")
        print(f"True Positives (TP): {results['true_positives (TP)']}")
        print(f"False Negatives (FN): {results['false_negatives (FN)']}")
        print(f"False Positives (FP): {results['false_positives (FP)']}")
        print("--------------------------------------------------")
        print(f"Precision: {results['precision']:.4f}")
        print(f"Recall: {results['recall']:.4f}")
        print(f"F1-Score: {results['f1_score']:.4f}")
        print("--------------------------------------------------")